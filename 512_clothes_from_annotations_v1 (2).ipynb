{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CveRAx5DiOXp",
        "outputId": "444e2cb2-9513-461d-95b1-7221d9c4fea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'UTFPR-SBD3'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 60 (delta 0), reused 3 (delta 0), pack-reused 56\u001b[K\n",
            "Receiving objects: 100% (60/60), 355.28 MiB | 9.38 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bioinfolabic/UTFPR-SBD3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3Jws0h7iR9M",
        "outputId": "5c9ef097-2039-43a4-aa6c-2c257c294099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/UTFPR-SBD3\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "%cd UTFPR-SBD3\n",
        "!python3 extract_dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xLvZiNn7hFO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data as utils\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAbqLHfADgUi",
        "outputId": "1aef1662-5c9b-44f2-ae74-f4f0f334df60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4500/4500 [00:33<00:00, 132.48it/s]\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import  tqdm\n",
        "os.makedirs(f'{os.getcwd()}/masked_images', exist_ok=True)\n",
        "for i in tqdm(os.listdir('images')):\n",
        "  img = Image.open(f'images/{i}')\n",
        "  img = np.array(img)\n",
        "  mask_name = i.replace('jpg','png')\n",
        "  mask = Image.open(f'masks/{mask_name}')\n",
        "  mask = np.array(mask)\n",
        "  img[mask==0] = 0\n",
        "  img = Image.fromarray(img)\n",
        "  img.save(f'masked_images/{i}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds2F-1Oi8Qwk"
      },
      "outputs": [],
      "source": [
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform_image=None,transform_mask=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_mask  = transform_mask\n",
        "        self.images_dir = os.path.join(root_dir, 'masked_images')  # Assuming images are in a folder named 'images'\n",
        "        self.masks_dir = os.path.join(root_dir, 'annotations')   # Assuming masks are in a folder named 'masks'\n",
        "        self.image_filenames = sorted(os.listdir(self.images_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(self.masks_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_name = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        mask = Image.open(mask_name).convert('L')  # 'L' mode for single-channel masks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if self.transform_image and self.transform_mask:\n",
        "            image = self.transform_image(image)\n",
        "            mask = self.transform_mask(mask)\n",
        "\n",
        "        return image,mask\n",
        "\n",
        "# Example usage:\n",
        "transform_image = transforms.Compose([transforms.Resize(size=(512, 512), interpolation=Image.NEAREST),transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform_mask = transforms.Compose([transforms.Resize(size=(512, 512), interpolation=Image.NEAREST),transforms.ToTensor()])\n",
        "dataset = SegmentationDataset(root_dir='/content/UTFPR-SBD3', transform_image=transform_image,transform_mask= transform_mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "disabGHpEi5L"
      },
      "outputs": [],
      "source": [
        "class SegmentationTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform_image=None,transform_mask=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_mask  = transform_mask\n",
        "        self.images_dir = os.path.join(root_dir, 'masked_images')  # Assuming images are in a folder named 'images'\n",
        "        self.masks_dir = os.path.join(root_dir, 'annotations')   # Assuming masks are in a folder named 'masks'\n",
        "        self.image_filenames = ['01967.jpg', '02145.jpg', '01407.jpg', '01918.jpg', '00342.jpg']\n",
        "        self.mask_filenames = ['01967.png', '02145.png', '01407.png', '01918.png', '00342.png']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.images_dir, self.image_filenames[idx])\n",
        "        mask_name = os.path.join(self.masks_dir, self.mask_filenames[idx])\n",
        "\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        mask = Image.open(mask_name).convert('L')  # 'L' mode for single-channel masks\n",
        "\n",
        "        if self.transform_image and self.transform_mask:\n",
        "            image = self.transform_image(image)\n",
        "            mask = self.transform_mask(mask)\n",
        "\n",
        "        return image,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewWr-sqlFH-r"
      },
      "outputs": [],
      "source": [
        "batch_size=1\n",
        "test_dataset = SegmentationTestDataset(root_dir='/content/UTFPR-SBD3', transform_image=transform_image,transform_mask= transform_mask)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQPJGL2a8Uqe"
      },
      "outputs": [],
      "source": [
        "batch_size=2\n",
        "train_dataloader= torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                             shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONh-zIYMfcxq"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv2d') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBgfpcXLoz57"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSnFxG5g0EAQ"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import functools\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class UnetGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
        "\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            up = [uprelu, upconv, nn.Tanh()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:   # add skip connections\n",
        "            return torch.cat([x, self.model(x)], 1)\n",
        "\n",
        "gen = UnetGenerator(1, 3, 8, 64, norm_layer=nn.BatchNorm2d, use_dropout=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBzBbEC8-5Wv"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(input_dim, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)),\n",
        "            nn.InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "disc = Discriminator(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ho9fewtoaeq"
      },
      "outputs": [],
      "source": [
        "# gen_opt = torch.optim.SGD(gen.parameters(), weight_decay=1e-4, lr = 0.001, momentum=0.9)\n",
        "# disc_opt = torch.optim.SGD(disc.parameters(), weight_decay=1e-4, lr = 0.0001, momentum=0.9)\n",
        "gen = gen.to(device)\n",
        "disc = disc.to(device)\n",
        "gen.apply(weights_init)\n",
        "disc.apply(weights_init)\n",
        "\n",
        "\n",
        "gen_opt = torch.optim.Adam(gen.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
        "disc_opt = torch.optim.Adam(disc.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
        "\n",
        "\n",
        "l1_loss = nn.L1Loss()\n",
        "bce_loss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXcwOc0FhBUQ"
      },
      "outputs": [],
      "source": [
        "def set_requires_grad(model, requires_grad=False):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4KPEiE63nlR",
        "outputId": "04b06b7b-5830-472d-dcd1-bcce05b36cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFc7etbTaTQz",
        "outputId": "6953e2ca-c7cb-4bd4-8967-5f756b210f1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen.load_state_dict(torch.load('/content/drive/MyDrive/gen_512_48.pth'))\n",
        "disc.load_state_dict(torch.load('/content/drive/MyDrive/disc_512_48.pth'))\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/UTFPR-SBD3/Test_results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofMNEvlXqCck"
      },
      "outputs": [],
      "source": [
        "epoch_start=49\n",
        "\n",
        "# checkpoint = torch.load('model.pt')\n",
        "# gen.load_state_dict(checkpoint['gen_state_dict'])\n",
        "# gen_opt.load_state_dict(checkpoint['genopt_state_dict'])\n",
        "# disc.load_state_dict(checkpoint['disc_state_dict'])\n",
        "# disc_opt.load_state_dict(checkpoint['discopt_state_dict'])\n",
        "# epoch_start = checkpoint['epoch']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmtLwhqZuYY1"
      },
      "outputs": [],
      "source": [
        "# new_learning_rate = 0.00002  # Set the new learning rate\n",
        "\n",
        "# # Update the learning rate of the optimizer\n",
        "# for param_group in disc_opt.param_groups:\n",
        "#     param_group['lr'] = new_learning_rate\n",
        "\n",
        "# for param_group in gen_opt.param_groups:\n",
        "#     param_group['lr'] = new_learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlw9Pg80ni4a",
        "outputId": "9ee7442c-ded5-4ba7-e9c8-5d5c3aa1384c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50], Discriminator Loss: 0.05142281949520111, Generator Loss: 4.250527858734131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [51], Discriminator Loss: 0.15634483098983765, Generator Loss: 3.9832491874694824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [52], Discriminator Loss: 0.07578960806131363, Generator Loss: 3.6275548934936523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [53], Discriminator Loss: 0.06491173803806305, Generator Loss: 4.302152156829834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [54], Discriminator Loss: 0.18794973194599152, Generator Loss: 3.1014275550842285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [55], Discriminator Loss: 0.13610419631004333, Generator Loss: 3.6736156940460205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [56], Discriminator Loss: 0.10339955985546112, Generator Loss: 3.3466906547546387\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [57], Discriminator Loss: 0.06339671462774277, Generator Loss: 4.975586891174316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:39<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [58], Discriminator Loss: 0.0782412439584732, Generator Loss: 4.855637550354004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:39<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [59], Discriminator Loss: 0.09039057046175003, Generator Loss: 4.125000476837158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:39<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [60], Discriminator Loss: 0.16032296419143677, Generator Loss: 3.3431930541992188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:39<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [61], Discriminator Loss: 0.1505441963672638, Generator Loss: 3.383056163787842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [62], Discriminator Loss: 0.15682171285152435, Generator Loss: 3.295691728591919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [63], Discriminator Loss: 0.10158611834049225, Generator Loss: 3.985898494720459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [64], Discriminator Loss: 0.07510465383529663, Generator Loss: 4.993413925170898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [65], Discriminator Loss: 0.21557122468948364, Generator Loss: 3.197904109954834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [66], Discriminator Loss: 0.11566153168678284, Generator Loss: 3.0970935821533203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [67], Discriminator Loss: 0.1746547818183899, Generator Loss: 3.397085428237915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [68], Discriminator Loss: 0.14645393192768097, Generator Loss: 3.4525327682495117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:39<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [69], Discriminator Loss: 0.06067577004432678, Generator Loss: 4.547272682189941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [70], Discriminator Loss: 0.10521315038204193, Generator Loss: 3.7592687606811523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [71], Discriminator Loss: 0.15120413899421692, Generator Loss: 3.2021265029907227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [72], Discriminator Loss: 0.06473126262426376, Generator Loss: 3.6148080825805664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [73], Discriminator Loss: 0.11658594012260437, Generator Loss: 5.493468761444092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [74], Discriminator Loss: 0.04914376884698868, Generator Loss: 5.448707580566406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:43<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [75], Discriminator Loss: 0.10962842404842377, Generator Loss: 3.81827974319458\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:43<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [76], Discriminator Loss: 0.14727994799613953, Generator Loss: 3.243567943572998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [77], Discriminator Loss: 0.2049919068813324, Generator Loss: 4.025151252746582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:40<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [78], Discriminator Loss: 0.1320689618587494, Generator Loss: 3.033667802810669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:43<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [79], Discriminator Loss: 0.16855354607105255, Generator Loss: 2.9431166648864746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:43<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [80], Discriminator Loss: 0.12965603172779083, Generator Loss: 3.991396427154541\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [81], Discriminator Loss: 0.06179840862751007, Generator Loss: 4.395103454589844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [82], Discriminator Loss: 0.16433444619178772, Generator Loss: 3.3071112632751465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [83], Discriminator Loss: 0.12375441938638687, Generator Loss: 3.8614988327026367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [84], Discriminator Loss: 0.11123117804527283, Generator Loss: 3.6917550563812256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [85], Discriminator Loss: 0.13066703081130981, Generator Loss: 4.206608772277832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [86], Discriminator Loss: 0.16848033666610718, Generator Loss: 3.0619077682495117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:41<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [87], Discriminator Loss: 0.10278636962175369, Generator Loss: 4.1566243171691895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:31<00:00,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [88], Discriminator Loss: 0.27841177582740784, Generator Loss: 2.886536121368408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2250/2250 [20:42<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [89], Discriminator Loss: 0.096564881503582, Generator Loss: 4.090121269226074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████▏   | 1383/2250 [12:42<07:59,  1.81it/s]"
          ]
        }
      ],
      "source": [
        "epochs=100\n",
        "os.makedirs('Test_results_up',exist_ok=True)\n",
        "for epoch in range(epoch_start,epochs+epoch_start):\n",
        "  gen.train()\n",
        "  disc.train()\n",
        "\n",
        "\n",
        "  for (images,masks) in tqdm(train_dataloader,total=len(train_dataloader)):\n",
        "    images,masks = images.to(device),masks.to(device)\n",
        "    # discriminator training\n",
        "    set_requires_grad(disc,requires_grad=True)\n",
        "    # set_requires_grad(gen,requires_grad=False)\n",
        "    disc_opt.zero_grad()\n",
        "    fake_images = gen(masks)\n",
        "    fake_images_conc = torch.concat([fake_images,masks],1)\n",
        "    real_images_conc = torch.concat([images,masks],1)\n",
        "\n",
        "    real_disc_out = disc(real_images_conc)\n",
        "    fake_disc_out = disc(fake_images_conc)\n",
        "\n",
        "    # real_labels = torch.ones_like(real_disc_out)- 0.1 * torch.rand_like(real_disc_out)\n",
        "    # fake_labels = torch.zeros_like(fake_disc_out) + 0.3 * torch.rand_like(fake_disc_out)\n",
        "\n",
        "    real_labels = torch.ones_like(real_disc_out).cuda()\n",
        "    fake_labels = torch.zeros_like(fake_disc_out).cuda()\n",
        "\n",
        "    # code of gradient penalty\n",
        "\n",
        "    # alpha = torch.rand(images.size(0), 1, 1, 1).to(device)\n",
        "    # interpolated = (alpha * images + (1 - alpha) * fake_images).requires_grad_(True)\n",
        "    # disc_interpolated = disc(torch.cat([interpolated, masks], 1))\n",
        "    # gradients = torch.autograd.grad(outputs=disc_interpolated, inputs=interpolated,\n",
        "    #                                 grad_outputs=torch.ones_like(disc_interpolated),\n",
        "    #                                 create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "    # gradient_penalty = lambda_gp * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "\n",
        "\n",
        "    real_disc_loss = bce_loss(real_disc_out,real_labels)\n",
        "    fake_disc_loss = bce_loss(fake_disc_out,fake_labels)\n",
        "\n",
        "    # total_disc_loss = (real_disc_loss + fake_disc_loss)*0.5 + gradient_penalty\n",
        "    total_disc_loss = (real_disc_loss + fake_disc_loss)*0.5\n",
        "    total_disc_loss.backward()\n",
        "    disc_opt.step()\n",
        "\n",
        "    # generator training\n",
        "    set_requires_grad(disc,requires_grad=False)\n",
        "    # set_requires_grad(gen,requires_grad=True)\n",
        "    gen_opt.zero_grad()\n",
        "    fake_images1 = gen(masks)\n",
        "\n",
        "    gen_loss = l1_loss(fake_images1,images)\n",
        "\n",
        "    fake_images_conc1 = torch.concat([fake_images1,masks],1)\n",
        "\n",
        "    disc_out = disc(fake_images_conc1)\n",
        "\n",
        "    disc_loss = bce_loss(disc_out,torch.ones_like(disc_out))\n",
        "\n",
        "    total_gen_loss = 100*gen_loss + disc_loss\n",
        "\n",
        "    total_gen_loss.backward()\n",
        "    gen_opt.step()\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    gen.eval()\n",
        "    for kk,(images,masks) in enumerate(test_dataloader):\n",
        "\n",
        "      images,masks = images.to(device),masks.to(device)\n",
        "      result = gen(masks)\n",
        "      output_tensor = result.squeeze()\n",
        "      denormalize = transforms.Normalize((-1, -1, -1), (2, 2, 2))\n",
        "      denormalized_tensor = denormalize(output_tensor)\n",
        "      output_numpy = denormalized_tensor.cpu().numpy()\n",
        "      output_image = Image.fromarray((output_numpy * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "      output_image.save(f'/content/UTFPR-SBD3/Test_results/{epoch}_{kk}_fake.jpg')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(f'Epoch [{epoch + 1}], Discriminator Loss: {total_disc_loss.item()}, Generator Loss: {total_gen_loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D2e1-9TMa4mY"
      },
      "outputs": [],
      "source": [
        "torch.save(gen.state_dict(), f'/content/drive/MyDrive/gen_512_100.pth')\n",
        "torch.save(disc.state_dict(), f'/content/drive/MyDrive/disc_512_100.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu6zcH8Dsbw6"
      },
      "outputs": [],
      "source": [
        "EPOCH = epoch\n",
        "PATH = \"model.pt\"\n",
        "\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'gen_state_dict': gen.state_dict(),\n",
        "            'genopt_state_dict': gen_opt.state_dict(),\n",
        "            'disc_state_dict': disc.state_dict(),\n",
        "            'discopt_state_dict': disc_opt.state_dict(),\n",
        "\n",
        "            }, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX8lm_hokQ4D"
      },
      "outputs": [],
      "source": [
        "test_dataset = SegmentationDataset('Test',transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4apt8t-bA4U"
      },
      "outputs": [],
      "source": [
        "# model = Custom(n_class=2)\n",
        "# model.load_state_dict(torch.load('point.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XukC3_ZgR7D8"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # torch.save(gen.state_dict(), f'/content/drive/MyDrive/gen_{epoch}.pth')\n",
        "    # torch.save(disc.state_dict(), f'/content/drive/MyDrive/disc_{epoch}.pth')\n",
        "    gen.eval()\n",
        "    for kk,(images,masks) in enumerate(test_dataloader):\n",
        "\n",
        "      images,masks = images.to(device),masks.to(device)\n",
        "      result = gen(masks)\n",
        "      output_tensor = result.squeeze()\n",
        "      denormalize = transforms.Normalize((-1, -1, -1), (2, 2, 2))\n",
        "      denormalized_tensor = denormalize(output_tensor)\n",
        "      output_numpy = denormalized_tensor.cpu().numpy()\n",
        "      # Rescale values to the original range [0, 1]\n",
        "      output_image = Image.fromarray((output_numpy * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "      # Show or save the resulting image\n",
        "      output_image.save(f'/content/UTFPR-SBD3/{kk}_fake.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xghrxAmJ8dsg"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/drive/MyDrive/clothes_unet_',exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x7j2nCZ8jqD"
      },
      "outputs": [],
      "source": [
        "torch.save(gen.state_dict(), f'/content/drive/MyDrive/clothes_unet_/gen_{epoch}.pth')\n",
        "torch.save(disc.state_dict(), f'/content/drive/MyDrive/clothes_unet_/disc_{epoch}.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu3ugPDf_OoN",
        "outputId": "7ec187c7-e10d-486f-8bd8-9591d5ed2a4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "disc.load_state_dict(torch.load('/content/drive/MyDrive/clothes_unet_/disc_99.pth'))\n",
        "gen.load_state_dict(torch.load('/content/drive/MyDrive/clothes_unet_/gen_99.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZMyWR81A_ED"
      },
      "outputs": [],
      "source": [
        "gen = gen.to(device)\n",
        "disc = disc.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N3BYqrKBwM1"
      },
      "outputs": [],
      "source": [
        "\n",
        "gen_drive_link : https://drive.google.com/file/d/1-1zNpaTmJLTc3sGeqzx-HKqc8s_9QiQu/view?usp=share_link\n",
        "disk_drive_link: https://drive.google.com/file/d/1-2KyHlw-q3x84UWG8SlwZs07ZEzRaHc8/view?usp=share_link"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gen_drive_link :"
      ],
      "metadata": {
        "id": "esuWgRj2xK9d"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}